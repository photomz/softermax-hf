# serializer version: 1
# name: test_trainingargs_bookcorpusnwiki
  TrainingArguments(
  _n_gpu=1,
  adafactor=False,
  adam_beta1=0.9,
  adam_beta2=0.999,
  adam_epsilon=1e-08,
  auto_find_batch_size=False,
  bf16=False,
  bf16_full_eval=False,
  data_seed=42,
  dataloader_drop_last=False,
  dataloader_num_workers=6,
  dataloader_persistent_workers=False,
  dataloader_pin_memory=True,
  ddp_backend=None,
  ddp_broadcast_buffers=None,
  ddp_bucket_cap_mb=None,
  ddp_find_unused_parameters=None,
  ddp_timeout=1800,
  debug=[],
  deepspeed=None,
  disable_tqdm=False,
  dispatch_batches=None,
  do_eval=False,
  do_predict=False,
  do_train=False,
  eval_accumulation_steps=None,
  eval_delay=0,
  eval_steps=None,
  evaluation_strategy=IntervalStrategy.NO,
  fp16=False,
  fp16_backend=auto,
  fp16_full_eval=False,
  fp16_opt_level=O1,
  fsdp=[],
  fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
  fsdp_min_num_params=0,
  fsdp_transformer_layer_cls_to_wrap=None,
  full_determinism=False,
  gradient_accumulation_steps=1,
  gradient_checkpointing=False,
  gradient_checkpointing_kwargs=None,
  greater_is_better=None,
  group_by_length=False,
  half_precision_backend=auto,
  hub_always_push=False,
  hub_model_id=None,
  hub_private_repo=False,
  hub_strategy=HubStrategy.EVERY_SAVE,
  hub_token=<HUB_TOKEN>,
  ignore_data_skip=False,
  include_inputs_for_metrics=False,
  include_num_input_tokens_seen=False,
  include_tokens_per_second=False,
  jit_mode_eval=False,
  label_names=None,
  label_smoothing_factor=0.0,
  learning_rate=5e-05,
  length_column_name=length,
  load_best_model_at_end=False,
  local_rank=0,
  log_level=passive,
  log_level_replica=warning,
  log_on_each_node=True,
  logging_dir=/tmp/runs,
  logging_first_step=False,
  logging_nan_inf_filter=True,
  logging_steps=500,
  logging_strategy=IntervalStrategy.STEPS,
  lr_scheduler_kwargs={},
  lr_scheduler_type=SchedulerType.LINEAR,
  max_grad_norm=1.0,
  max_steps=100000,
  metric_for_best_model=None,
  mp_parameters=,
  neftune_noise_alpha=None,
  no_cuda=False,
  num_train_epochs=3.0,
  optim=OptimizerNames.ADAMW_TORCH,
  optim_args=None,
  output_dir=/tmp,
  overwrite_output_dir=False,
  past_index=-1,
  per_device_eval_batch_size=32,
  per_device_train_batch_size=32,
  prediction_loss_only=False,
  push_to_hub=False,
  push_to_hub_model_id=None,
  push_to_hub_organization=None,
  push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
  ray_scope=last,
  remove_unused_columns=True,
  report_to=[],
  resume_from_checkpoint=None,
  run_name=/tmp,
  save_on_each_node=False,
  save_only_model=False,
  save_safetensors=True,
  save_steps=500,
  save_strategy=IntervalStrategy.STEPS,
  save_total_limit=None,
  seed=42,
  skip_memory_metrics=True,
  split_batches=False,
  tf32=None,
  torch_compile=False,
  torch_compile_backend=None,
  torch_compile_mode=None,
  torchdynamo=None,
  tpu_metrics_debug=False,
  tpu_num_cores=None,
  use_cpu=False,
  use_ipex=False,
  use_legacy_prediction_loop=False,
  use_mps_device=False,
  warmup_ratio=0.0,
  warmup_steps=0,
  weight_decay=0.0,
  )
# ---
# name: test_trainloader_bookcorpusnwiki
  Size(
    32,
    128,
  )
# ---
# name: test_trainloader_bookcorpusnwiki.1
  {'input_ids': tensor([[    0,  8877,   128,  ...,     1,     1,     1],
          [    0,    90, 12541,  ...,     1,     1,     1],
          [    0, 50264,    42,  ...,     1,     1,     1],
          ...,
          [    0,   627,  4382,  ...,     1,     1,     1],
          [    0, 49519,   939,  ...,     1,     1,     1],
          [    0, 49519, 14223,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
          [1, 1, 1,  ..., 0, 0, 0],
          [1, 1, 1,  ..., 0, 0, 0],
          ...,
          [1, 1, 1,  ..., 0, 0, 0],
          [1, 1, 1,  ..., 0, 0, 0],
          [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],
          [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
          [ -100, 49519,  -100,  ...,  -100,  -100,  -100],
          ...,
          [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
          [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
          [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])}
# ---
# name: test_trainset_bookcorpusnwiki
  Size(
    128,
  )
# ---
# name: test_trainset_bookcorpusnwiki.1
  dict({
    'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0]),
    'input_ids': tensor([    0,  8877,   128,   417,   393,    56,    10,   313, 15256,   215,
           5676,  6453,    13,    69,   479,     2,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1]),
    'special_tokens_mask': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1]),
  })
# ---
